# ì„ë² ë”© ìƒì„± ë° ì €ì¥(ì¼ë‹¨ í‰ê·  ì•™ìƒë¸”)
import json
import numpy as np
from sentence_transformers import SentenceTransformer
import os
from datetime import datetime

# ---------- 1. JSON ê²½ë¡œ ì§ì ‘ ì…ë ¥ ----------
json_path = "keyword/keywords_20251028_130303.json"

if not os.path.exists(json_path):
    raise FileNotFoundError(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {json_path}")

with open(json_path, encoding="utf-8") as f:
    data = json.load(f)

# ---------- 2. í‚¤ì›Œë“œ ì½ê¸° ----------
korean_words = [kw["korean"] for kw in data.get("keywords", [])]
english_words = [kw["english"] for kw in data.get("keywords", [])]

if not korean_words or not english_words:
    raise ValueError("âŒ JSON íŒŒì¼ì—ì„œ 'keywords' í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

print(f"\nğŸ“ ë¶ˆëŸ¬ì˜¨ í‚¤ì›Œë“œ íŒŒì¼: {json_path}")
print(f"í•œêµ­ì–´ í‚¤ì›Œë“œ: {korean_words}")
print(f"ì˜ì–´ í‚¤ì›Œë“œ: {english_words}")

# ---------- 3. ëª¨ë¸ ë¡œë“œ ----------
print("\nğŸš€ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘...")
model_en = SentenceTransformer("all-MiniLM-L6-v2")              # ì˜ì–´ìš©
model_ko = SentenceTransformer("jhgan/ko-sroberta-multitask")  # í•œêµ­ì–´ìš©

# ---------- 4. ì„ë² ë”© ìƒì„± ----------
# í•œêµ­ì–´ì™€ ì˜ì–´ í‚¤ì›Œë“œë¥¼ í•˜ë‚˜ë¡œ í•©ì¹©ë‹ˆë‹¤.
all_keywords_text = " ".join(korean_words + english_words)
# ì˜ˆ: "í–‰ë³µí•œ ì‹ ë‚˜ëŠ” happy exciting"

print(f"ì•™ìƒë¸”ìš© í†µí•© í…ìŠ¤íŠ¸: {all_keywords_text}")

# "í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸"ë¥¼ ë‘ ëª¨ë¸ì— ëª¨ë‘ ì…ë ¥í•©ë‹ˆë‹¤.
emb_en = model_en.encode([all_keywords_text]) # (1, 384)
emb_ko = model_ko.encode([all_keywords_text]) # (1, 768)

# ì°¨ì› ë§ì¶”ê¸° ìœ„í•´ zero padding (KoBERT:768 ê¸°ì¤€)
if emb_en.shape[1] != emb_ko.shape[1]:
    max_dim = max(emb_en.shape[1], emb_ko.shape[1]) # 768
    padded_en = np.pad(emb_en, ((0, 0), (0, max_dim - emb_en.shape[1])))
    padded_ko = np.pad(emb_ko, ((0, 0), (0, max_dim - emb_ko.shape[1])))
else:
    padded_en, padded_ko = emb_en, emb_ko

# í‰ê·  ì•™ìƒë¸”
emb_concat = (padded_en + padded_ko) / 2

# ---------- 5. ì €ì¥ ----------
timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
os.makedirs("embeddings", exist_ok=True)
save_path = f"embeddings/embedding_{timestamp}.npy"
np.save(save_path, emb_concat)

# ---------- 6. ë©”íƒ€ ì •ë³´ ì €ì¥ ----------
meta = {
    "source_json": json_path,
    "timestamp": timestamp,
    "korean_keywords": korean_words,
    "english_keywords": english_words,
    "embedding_path": save_path
}

meta_path = f"embeddings/meta_{timestamp}.json"
with open(meta_path, "w", encoding="utf-8") as f:
    json.dump(meta, f, ensure_ascii=False, indent=2)

print(f"\nâœ… ì„ë² ë”© ì €ì¥ ì™„ë£Œ â†’ {save_path}")
print(f"ğŸ§¾ ë©”íƒ€ ì •ë³´ ì €ì¥ ì™„ë£Œ â†’ {meta_path}")


SAVE_DIR = "embeddings"
os.makedirs(SAVE_DIR, exist_ok=True)