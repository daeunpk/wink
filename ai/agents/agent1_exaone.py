# -*- coding: utf-8 -*-
# agent 1:í•œêµ­ì–´ ì…ë ¥ â†’ ì˜ì–´ ì¶œë ¥
# agent 3-3:ì˜ì–´ í‚¤ì›Œë“œ ì…ë ¥ â†’ í•œêµ­ì–´ í‚¤ì›Œë“œ

"""
EXAONE Agent (Module)
- EXAONE ëª¨ë¸ì˜ ìºì‹œ ë¡œë“œë¥¼ ê´€ë¦¬í•©ë‹ˆë‹¤.
- (Agent 1) í•œêµ­ì–´ -> ì˜ì–´ ë²ˆì—­ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
- (Agent 3-3) ì˜ì–´ -> í•œêµ­ì–´ ë²ˆì—­ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.
"""

import re
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

# =========================================================
# 1. ëª¨ë¸ ì„¤ì •
# =========================================================
MODEL_NAME = "LGAI-EXAONE/EXAONE-4.0-1.2B"

# =========================================================
# 2. EXAONE ëª¨ë¸ ìºì‹œ ë¡œë“œ (Agent 3ì˜ ë°©ì‹ì„ ê°€ì ¸ì˜´)
# =========================================================
_exa_tok, _exa_model = None, None
def _load_exaone():
    """
    EXAONE ëª¨ë¸ì„ í•œ ë²ˆë§Œ ë¡œë“œí•˜ì—¬ VRAMì„ ì ˆì•½í•˜ëŠ” ìºì‹œ í•¨ìˆ˜.
    """
    global _exa_tok, _exa_model
    if _exa_tok is None or _exa_model is None:
        print(f"ğŸ”„ Loading EXAONE model ({MODEL_NAME})...")
        _exa_tok = AutoTokenizer.from_pretrained(MODEL_NAME)
        _exa_model = AutoModelForCausalLM.from_pretrained(
            MODEL_NAME, torch_dtype=torch.bfloat16, device_map="auto"
        )
    return _exa_tok, _exa_model

# =========================================================
# 3. [Agent 1 ê¸°ëŠ¥] í•œêµ­ì–´ â†’ ì˜ì–´
# =========================================================
def korean_to_english(korean_text: str) -> str:
    """
    í•œêµ­ì–´ í…ìŠ¤íŠ¸ë¥¼ ìì—°ìŠ¤ëŸ¬ìš´ ì˜ì–´ ë¬¸ì¥ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    """
    if not korean_text.strip():
        return ""
    print("ğŸ§  [Agent 1] Translating Korean â†’ English (EXAONE)...")
    tok, mdl = _load_exaone() # ìºì‹œëœ ëª¨ë¸ ì‚¬ìš©

    messages = [
        {"role": "user", "content": f"Translate the following Korean text into one natural English sentence. Respond *only* with the translated sentence itself, without any explanations or conversational text.\n\nKorean: {korean_text}"}
    ]
    inputs = tok.apply_chat_template(
        messages, return_tensors="pt", add_generation_prompt=True
    ).to(mdl.device)

    input_length = inputs.shape[1]
    with torch.no_grad():
        outputs = mdl.generate(inputs, max_new_tokens=256, do_sample=False)
    
    new_tokens = outputs[0][input_length:]
    result_text = tok.decode(new_tokens, skip_special_tokens=True).strip()

    match = re.search(r'["\'](.*?_*)["\']', result_text)
    if match:
        return match.group(1).strip()
    return result_text.split('\n')[-1].strip()

# =========================================================
# 5. í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (Agent 1ì˜ ì›ë³¸ í…ŒìŠ¤íŠ¸ ì½”ë“œ)
# =========================================================
if __name__ == "__main__":
    """
    ì´ íŒŒì¼ì„ ì§ì ‘ ì‹¤í–‰í•  ê²½ìš°, Agent 1 ê¸°ëŠ¥ë§Œ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
    """
    try:
        print("--- EXAONE Agent (Module) Test ---")
        text_ko = input("\ní•œêµ­ì–´ ë¬¸ì¥ì„ ì…ë ¥í•˜ì„¸ìš”: ").strip()
        if not text_ko:
            print("âŒ ì…ë ¥ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
        else:
            result = korean_to_english(text_ko)
            print("\nğŸŒ ë³€í™˜ëœ ì˜ì–´ ë¬¸ì¥:")
            print(result)
    except KeyboardInterrupt:
        print("\nğŸ›‘ í”„ë¡œê·¸ë¨ ì¢…ë£Œë¨.")