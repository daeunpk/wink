# -*- coding: utf-8 -*-
"""
Agent3 (Session-based Pipeline)
- ì‚¬ìš©ì ì…ë ¥(í•œêµ­ì–´ í…ìŠ¤íŠ¸, ì´ë¯¸ì§€ ê²½ë¡œ)ì„ ì§ì ‘ ë°›ìŒ
- ë‚´ë¶€ì ìœ¼ë¡œ Agent1(í•œêµ­ì–´â†’ì˜ì–´), Agent2(ì´ë¯¸ì§€â†’ì˜ì–´ ìº¡ì…˜)
- Gemma3: ë‘ ê²°ê³¼ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ í•˜ë‚˜ì˜ ë¬¸ì¥ìœ¼ë¡œ ì¬ì‘ì„±
- Gemma3: ì˜ì–´ í‚¤ì›Œë“œ 5ê°œ ì¶”ì¶œ
- EXAONE: ì˜ì–´ í‚¤ì›Œë“œë¥¼ í•œêµ­ì–´ë¡œ ë²ˆì—­
- ëª¨ë“  ì‹¤í–‰ ê²°ê³¼ë¥¼ ì„¸ì…˜ ë‹¨ìœ„ JSONì— ëˆ„ì  ì €ì¥
"""

import os
import re
import json
import base64
from datetime import datetime
import requests
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

# =========================================================
# 1. ì „ì—­ ì„¤ì •
# =========================================================
OLLAMA_URL = "http://localhost:11434"
GEMMA3_MODEL = "gemma3:27b"
EXAONE_MODEL = "LGAI-EXAONE/EXAONE-4.0-1.2B"
SAVE_DIR = "agents/keywords"
os.makedirs(SAVE_DIR, exist_ok=True)

# =========================================================
# 2. EXAONE ëª¨ë¸ ìºì‹œ ë¡œë“œ
# =========================================================
_exa_tok, _exa_model = None, None
def _load_exaone():
    global _exa_tok, _exa_model
    if _exa_tok is None or _exa_model is None:
        print("ğŸ”„ Loading EXAONE model...")
        _exa_tok = AutoTokenizer.from_pretrained(EXAONE_MODEL)
        _exa_model = AutoModelForCausalLM.from_pretrained(
            EXAONE_MODEL, torch_dtype="bfloat16", device_map="auto"
        )
    return _exa_tok, _exa_model

# =========================================================
# 3. í•œêµ­ì–´ â†’ ì˜ì–´ (EXAONE) - (ë²„ê·¸ ìˆ˜ì •)
# =========================================================
def korean_to_english(korean_text: str) -> str:
    if not korean_text.strip():
        return ""
    print("ğŸ§  Translating Korean â†’ English ...")
    tok, mdl = _load_exaone()

    # [ìˆ˜ì •] Chat í…œí”Œë¦¿ ì ìš©
    messages = [
        {"role": "user", "content": f"Translate this Korean sentence into natural English:\n{korean_text}"}
    ]
    inputs = tok.apply_chat_template(
        messages,
        return_tensors="pt",
        add_generation_prompt=True # <|assistant|> í”„ë¡¬í”„íŠ¸ ì¶”ê°€
    ).to(mdl.device)

    # [ìˆ˜ì •] í”„ë¡¬í”„íŠ¸ ê¸¸ì´ë¥¼ ì œì™¸í•˜ê³  'ìƒˆë¡œ ìƒì„±ëœ í† í°'ë§Œ ë””ì½”ë”©
    input_length = inputs.shape[1]
    with torch.no_grad():
        outputs = mdl.generate(inputs, max_new_tokens=256, do_sample=False)
    
    new_tokens = outputs[0][input_length:]
    result_text = tok.decode(new_tokens, skip_special_tokens=True).strip()
    
    return result_text

# =========================================================
# 4. ì´ë¯¸ì§€ â†’ ì˜ì–´ ìº¡ì…˜ (Gemma3 via Ollama)
# =========================================================
def image_to_english_caption(image_path: str) -> str:
    if not image_path or not os.path.exists(image_path):
        return ""
    print("ğŸ–¼ï¸ Describing image â†’ English caption ...")

    with open(image_path, "rb") as f:
        image_b64 = base64.b64encode(f.read()).decode("utf-8")

    prompt = "Describe this image in ONE English sentence focusing on mood and atmosphere."
    payload = {"model": GEMMA3_MODEL, "prompt": prompt, "images": [image_b64], "stream": False}
    try:
        res = requests.post(f"{OLLAMA_URL}/api/generate", json=payload, timeout=120)
        res.raise_for_status()
        return (res.json().get("response") or "").strip()
    except Exception as e:
        print(f"âš ï¸ Image caption generation failed: {e}")
        return ""

# =========================================================
# 5. ë‘ ì˜ì–´ ë¬¸ì¥ í•©ì¹˜ê¸° (Gemma3)
# =========================================================
def rewrite_combined_sentence(text1: str, text2: str) -> str:
    if not (text1 or text2):
        return ""
    print("ğŸ§© Merging English sentences ...")

    prompt = f"""
Combine the following two English sentences into ONE smooth, natural descriptive sentence. 
Preserve their emotional and atmospheric tone.

Sentence 1: {text1}
Sentence 2: {text2}
"""
    payload = {"model": GEMMA3_MODEL, "prompt": prompt.strip(), "stream": False}
    try:
        res = requests.post(f"{OLLAMA_URL}/api/generate", json=payload, timeout=60)
        res.raise_for_status()
        return (res.json().get("response") or "").strip()
    except Exception as e:
        print(f"âš ï¸ Merge failed: {e}")
        return f"{text1} {text2}".strip()

# =========================================================
# 6. ê°ì„± í‚¤ì›Œë“œ ì¶”ì¶œ (Gemma3)
# =========================================================
def extract_keywords(merged_text: str, k: int = 5) -> list[str]:
    if not merged_text.strip():
        return []
    print("ğŸ’¬ Extracting mood keywords ...")

    prompt = f"""
From the text below, extract exactly {k} concise single-word adjectives that describe the mood or tone.
Output only a comma-separated list.

Text:
{merged_text}
"""
    payload = {"model": GEMMA3_MODEL, "prompt": prompt.strip(), "stream": False}
    try:
        res = requests.post(f"{OLLAMA_URL}/api/generate", json=payload, timeout=60)
        res.raise_for_status()
        raw = (res.json().get("response") or "").strip()
        words = [re.sub(r"[^a-z\-]", "", w.strip().lower()) for w in raw.split(",")]
        return [w for w in words if w][:k]
    except Exception as e:
        print(f"ğŸ”¥ Keyword extraction failed: {e}")
        return []

# =========================================================
# 7. ì˜ì–´ í‚¤ì›Œë“œ â†’ í•œêµ­ì–´ (EXAONE) - (ë²„ê·¸ ìˆ˜ì •)
# =========================================================
def translate_keywords_to_korean(english_keywords: list[str]) -> list[str]:
    """
    ì˜ì–´ ê°ì„± í‚¤ì›Œë“œë¥¼ í•œêµ­ì–´ë¡œ ë²ˆì—­ (EXAONE, ì™„ì „ ê°œì„  ë²„ì „)
    """
    if not english_keywords:
        return []

    print("ğŸŒ Translating English keywords â†’ Korean (EXAONE)...")
    tok, mdl = _load_exaone()

    # [ìˆ˜ì •] Chat í…œí”Œë¦¿ ì ìš©
    prompt_content = (
        "ë‹¤ìŒ ì˜ì–´ í˜•ìš©ì‚¬ë“¤ì„ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ ë‹¨ì–´ë¡œ ë²ˆì—­í•˜ì‹œì˜¤.\n"
        "ì¶œë ¥ì€ ì‰¼í‘œ(,)ë¡œ êµ¬ë¶„ëœ í•œê¸€ ë‹¨ì–´ë§Œ í¬í•¨í•˜ì‹œì˜¤.\n\n"
        f"{', '.join(english_keywords)}"
    )
    messages = [
        {"role": "user", "content": prompt_content}
    ]
    inputs = tok.apply_chat_template(
        messages,
        return_tensors="pt",
        add_generation_prompt=True
    ).to(mdl.device)

    # [ìˆ˜ì •] í”„ë¡¬í”„íŠ¸ ê¸¸ì´ë¥¼ ì œì™¸í•˜ê³  'ìƒˆë¡œ ìƒì„±ëœ í† í°'ë§Œ ë””ì½”ë”©
    input_length = inputs.shape[1]
    with torch.no_grad():
        outputs = mdl.generate(inputs, max_new_tokens=128, do_sample=False)
    
    new_tokens = outputs[0][input_length:]
    translated = tok.decode(new_tokens, skip_special_tokens=True).strip()

    # [ìˆ˜ì •] ëª¨ë¸ì´ í”„ë¡¬í”„íŠ¸ë¥¼ ë°˜ë³µí•˜ì§€ ì•Šë„ë¡, ì‘ë‹µì—ì„œ í•œê¸€/ì‰¼í‘œë§Œ ì¶”ì¶œ
    ko_words = re.findall(r"[\u3131-\ucb4f]+", translated) # ì •ê·œì‹ìœ¼ë¡œ í•œê¸€ ë‹¨ì–´ë§Œ ì¶”ì¶œ

    # ë§Œì•½ ëª¨ë¸ì´ "ìœ¼ìŠ¤ìŠ¤í•œ, ì™¸ë¡œìš´, ..." ì²˜ëŸ¼ ì‰¼í‘œë¡œ ì˜ ë°˜í™˜í–ˆì„ ê²½ìš°
    if not ko_words or len(ko_words) < len(english_keywords):
        ko_words = [w.strip() for w in re.split(r"[,ï¼Œ\s\n]+", translated) if w.strip()]
        
    final_list = [w for w in ko_words if not w.isascii() and w not in ["ë‹¤ìŒ", "ì˜ì–´", "í˜•ìš©ì‚¬ë“¤ì„"]] # í”„ë¡¬í”„íŠ¸ ë‹¨ì–´ í•„í„°ë§
    
    if len(final_list) >= len(english_keywords):
        return final_list[:len(english_keywords)]
    else:
        # ë²ˆì—­ ì‹¤íŒ¨ ì‹œ
        print(f"âš ï¸ KO translation parsing failed. Raw output: {translated}")
        return (final_list + ["ë²ˆì—­ì‹¤íŒ¨"] * len(english_keywords))[:len(english_keywords)]
    
# =========================================================
# 8. ì„¸ì…˜ ì €ì¥ (ì•ˆì „í•œ ë²„ì „)
# =========================================================
def save_to_session_simple(data: dict, session_file: str):
    """
    ì§€ì •ëœ ì„¸ì…˜ JSON íŒŒì¼ì„ ì•ˆì „í•˜ê²Œ ì—´ê³ , ë°ì´í„°ë¥¼ appendí•©ë‹ˆë‹¤.
    íŒŒì¼ì´ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.
    """
    
    # 1. ìƒˆ ì„¸ì…˜ì˜ ê¸°ë³¸ êµ¬ì¡° ì •ì˜
    default_structure = {
        "session_name": os.path.basename(session_file).replace(".json", ""),
        "session_start": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "input_korean": [],
        "input_image": [],
        "english_text_from_agent1": [],
        "english_caption_from_agent2": [],
        "merged_sentence": [],
        "english_keywords": [],
        "korean_keywords": []
    }
    
    if os.path.exists(session_file):
        try:
            with open(session_file, "r", encoding="utf-8") as f:
                session_data = json.load(f)
            
            # [ì•ˆì „ì¥ì¹˜] ğŸ‘ˆ ê¸°ì¡´ íŒŒì¼ì— í‚¤ê°€ ëˆ„ë½ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ê³  ì¶”ê°€
            for key, default_value in default_structure.items():
                if key not in session_data:
                    print(f"âš ï¸ ê¸°ì¡´ ì„¸ì…˜ íŒŒì¼ì— '{key}' í‚¤ê°€ ì—†ì–´ ì¶”ê°€í•©ë‹ˆë‹¤.")
                    session_data[key] = default_value
                    
        except json.JSONDecodeError:
            print(f"âš ï¸ ì„¸ì…˜ íŒŒì¼ì´ ì†ìƒë˜ì–´ ìƒˆë¡œ ë§Œë“­ë‹ˆë‹¤: {session_file}")
            session_data = default_structure # ì†ìƒ ì‹œ ê¸°ë³¸ êµ¬ì¡°ë¡œ ë®ì–´ì“°ê¸°
    else:
        # íŒŒì¼ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ êµ¬ì¡° ì‚¬ìš©
        session_data = default_structure

    # 2. ì´ì œ ëª¨ë“  í‚¤ê°€ ì¡´ì¬í•˜ë¯€ë¡œ ì•ˆì „í•˜ê²Œ append
    try:
        session_data["input_korean"].append(data["input"]["korean_text"])
        session_data["input_image"].append(data["input"]["image_path"])
        session_data["english_text_from_agent1"].append(data["english_text_from_agent1"])
        session_data["english_caption_from_agent2"].append(data["english_caption_from_agent2"])
        session_data["merged_sentence"].append(data["merged_sentence"])
        session_data["english_keywords"].append(data["english_keywords"])
        session_data["korean_keywords"].append(data["korean_keywords"])
    except KeyError as e:
        print(f"ğŸ”¥ ë°ì´í„° ì €ì¥ ì¤‘ ì¹˜ëª…ì ì¸ Key Error ë°œìƒ: {e}")
        return

    # 3. íŒŒì¼ ì“°ê¸°
    with open(session_file, "w", encoding="utf-8") as f:
        json.dump(session_data, f, ensure_ascii=False, indent=2)
                
# =========================================================
# 9. ì „ì²´ ì‹¤í–‰ íŒŒì´í”„ë¼ì¸
# =========================================================
def run_agent3_session(korean_text="", image_path="", session_name="current_session"):
    english_text = korean_to_english(korean_text) if korean_text else ""
    english_caption = image_to_english_caption(image_path) if image_path else ""
    merged = rewrite_combined_sentence(english_text, english_caption)
    eng_keywords = extract_keywords(merged)
    kor_keywords = translate_keywords_to_korean(eng_keywords)

    data = {
        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "input": {"korean_text": korean_text, "image_path": image_path},
        "english_text_from_agent1": english_text,
        "english_caption_from_agent2": english_caption,
        "merged_sentence": merged,
        "english_keywords": eng_keywords,
        "korean_keywords": kor_keywords
    }

    session_file = os.path.join(SAVE_DIR, f"{session_name}.json")
    save_to_session_simple(data, session_file)
    print(f"\nâœ… Saved to session â†’ {session_file}")
    return data

# =========================================================
# 10. CLI (ì„¸ì…˜ ê´€ë¦¬ì)
# =========================================================
if __name__ == "__main__":
    print("\nğŸ¤– Agent3 ì„¸ì…˜í˜• ì‹¤í–‰ (í•œêµ­ì–´ í…ìŠ¤íŠ¸/ì´ë¯¸ì§€ ì…ë ¥)")
    
    # --- 1. ê¸°ì¡´ ì„¸ì…˜ íŒŒì¼ ëª©ë¡ ë¶ˆëŸ¬ì˜¤ê¸° ---
    session_files = [f for f in os.listdir(SAVE_DIR) if f.endswith('.json')]
    session_files.sort(reverse=True) # ìµœê·¼ ì„¸ì…˜ì´ ìœ„ë¡œ ì˜¤ë„ë¡ ì •ë ¬
    
    session_name = ""

    if not session_files:
        print("   -> ê¸°ì¡´ ì„¸ì…˜ì´ ì—†ìŠµë‹ˆë‹¤. 'new'ë¥¼ ì„ íƒí•˜ì„¸ìš”.")
    else:
        print("\n--- ğŸ—‚ï¸  ê¸°ì¡´ ì„¸ì…˜ ëª©ë¡ ---")
        for i, f_name in enumerate(session_files):
            print(f"   [{i+1}] {f_name}")
        print("--------------------------")

    # --- 2. ì„¸ì…˜ ì„ íƒ ---
    choice = input("\nìƒˆ ëŒ€í™”ëŠ” 'new', ì´ì–´í•˜ê¸°ëŠ” 'ë²ˆí˜¸' ì…ë ¥: ").strip().lower()

    if choice == "new":
        session_name = f"session_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        print(f"   -> ğŸ†• ìƒˆ ì„¸ì…˜ì„ ì‹œì‘í•©ë‹ˆë‹¤: {session_name}.json")
    elif choice.isdigit() and 1 <= int(choice) <= len(session_files):
        # ì‚¬ìš©ìê°€ 1ì„ ì…ë ¥í•˜ë©´ ë¦¬ìŠ¤íŠ¸ì˜ 0ë²ˆì§¸ íŒŒì¼ ì„ íƒ
        selected_file = session_files[int(choice) - 1]
        session_name = selected_file.replace(".json", "") # '.json' í™•ì¥ì ì œê±°
        print(f"   -> â¡ï¸ ê¸°ì¡´ ì„¸ì…˜ì— ì´ì–´í•©ë‹ˆë‹¤: {session_name}.json")
    else:
        print("   -> âš ï¸ ì˜ëª»ëœ ì…ë ¥ì…ë‹ˆë‹¤. ìƒˆ ì„¸ì…˜ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
        session_name = f"session_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        print(f"   -> ğŸ†• ìƒˆ ì„¸ì…˜ì„ ì‹œì‘í•©ë‹ˆë‹¤: {session_name}.json")

    # --- 3. ì‚¬ìš©ì ì…ë ¥ ë°›ê¸° ---
    print("\n--- ğŸ’¬ ì…ë ¥ì„ ì‹œì‘í•˜ì„¸ìš” ---")
    text = input("í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì…ë ¥ (ì—†ìœ¼ë©´ Enter): ").strip()
    img = input("ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ ì…ë ¥ (ì—†ìœ¼ë©´ Enter): ").strip()

    if not text and not img:
        print("\nğŸ›‘ ì…ë ¥ì´ ì—†ì–´ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        exit()

    # --- 4. íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ---
    print("\n--- ğŸš€ ì—ì´ì „íŠ¸ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ---")
    try:
        result = run_agent3_session(text, img, session_name=session_name)
        
        print("\n--- ğŸ¯ ì‹¤í–‰ ê²°ê³¼ ---")
        print(json.dumps(result, ensure_ascii=False, indent=2))
        
    except Exception as e:
        print(f"\nğŸ”¥ğŸ”¥ğŸ”¥ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
        # (ì˜ˆ: EXAONE ë¡œë“œ ì‹¤íŒ¨, Ollama ì—°ê²° ì‹¤íŒ¨ ë“±)